{{- if .Values.prometheus.rule.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "frontend.fullname" . }}-rules
  labels:
    release: monitoring
    {{- include "frontend.labels" . | nindent 4 }}
spec:
  groups:
  - name: frontend
    rules:
    # Compute 5xx error rate for the frontend
    - record: job:http_5xx_rate:ratio
      expr: |
        sum(rate(http_requests_total{job="frontend",code=~"5.."}[5m]))
        /
        sum(rate(http_requests_total{job="frontend"}[5m]))
    # Page when the 5xx rate is high for a few minutes
    - alert: FrontendHighErrorRate
      expr: job:http_5xx_rate:ratio > {{ .Values.prometheus.rule.errorRateThreshold }}
      for: 5m
      labels: { severity: critical, service: frontend, env: local }
      annotations:
        summary: "Frontend 5xx > {{ .Values.prometheus.rule.errorRateThreshold }} for 5m"
    # Availability (alert if all pods are down)
    - alert: FrontendDown
      expr: up{job="frontend"} == 0 or absent(up{job="frontend"}) == 1
      for: 2m
      labels: { severity: critical, service: frontend, env: local }
      annotations:
        summary: "Frontend is not responding (no scrape targets UP)"
    - alert: FrontendHighMemoryUsage
      expr: container_memory_usage_bytes{pod=~"frontend-.*"} / container_spec_memory_limit_bytes > 0.8
      for: 5m
      labels: { severity: warning, service: frontend, env: local }
      annotations:
        summary: "Frontend pod memory usage > 80%"
    - alert: FrontendPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total{pod=~"frontend-.*"}[15m]) > 0
      for: 5m
      labels: { severity: critical, service: frontend, env: local }
      annotations:
        summary: "Frontend pod is crash looping"
{{- end }}
